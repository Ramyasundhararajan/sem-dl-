{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8482,"sourceType":"datasetVersion","datasetId":5689},{"sourceId":163621,"sourceType":"datasetVersion","datasetId":73247},{"sourceId":2020792,"sourceType":"datasetVersion","datasetId":1209510},{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310},{"sourceId":9659344,"sourceType":"datasetVersion","datasetId":5901195},{"sourceId":2859008,"sourceType":"kernelVersion"},{"sourceId":55281833,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nimport re\nimport string\n\n# Step 1: Load the dataset\n# Replace 'path_to_dataset/reviews.csv' with the actual path to the IMDb dataset.\ndf = pd.read_csv('/kaggle/input/movie-review/labelled_full_dataset.csv')\n\n# Check the structure of the dataframe\nprint(df.head())\n\n# Step 2: Data Preprocessing\n# Clean text function\ndef clean_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Convert to lowercase\n    text = text.lower()\n    # Remove digits\n    text = re.sub(r'\\d+', '', text)\n    return text\n\n# Apply cleaning to the 'review' column\ndf['cleaned_text'] = df['review'].apply(clean_text)\n\n# Step 3: Convert text data into numerical vectors\nvectorizer = TfidfVectorizer(max_features=5000)  # Use TF-IDF for vectorization\nX = vectorizer.fit_transform(df['cleaned_text'])\ny = df['label']  # Use the 'label' column for sentiment labels\n\n# Step 4: Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 5: Build and train the logistic regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Step 6: Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\nprint(classification_report(y_test, y_pred))\n\n# Sample Input (Review Text)\nsample_review = \"This movie was amazing! I loved every minute of it.\"\nsample_cleaned = clean_text(sample_review)\nsample_vectorized = vectorizer.transform([sample_cleaned])\n\n# Predict sentiment for the sample input\nsample_prediction = model.predict(sample_vectorized)\nprint(f'Sample Review Sentiment: {\"Positive\" if sample_prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:15:33.111749Z","iopub.execute_input":"2024-09-25T12:15:33.112490Z","iopub.status.idle":"2024-09-25T12:15:49.447495Z","shell.execute_reply.started":"2024-09-25T12:15:33.112432Z","shell.execute_reply":"2024-09-25T12:15:49.446351Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"   label                                             review\n0      0  Once again Mr. Costner has dragged out a movie...\n1      0  This is an example of why the majority of acti...\n2      0  First of all I hate those moronic rappers, who...\n3      0  Not even the Beatles could write songs everyon...\n4      0  Brass pictures (movies is not a fitting word f...\nAccuracy: 0.89\n              precision    recall  f1-score   support\n\n           0       0.90      0.88      0.89      5022\n           1       0.88      0.90      0.89      4978\n\n    accuracy                           0.89     10000\n   macro avg       0.89      0.89      0.89     10000\nweighted avg       0.89      0.89      0.89     10000\n\nSample Review Sentiment: Positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"B. Twitter Sentiment Analysis using LSTM & GloVe Embeddings","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Step 1: Load and preprocess the dataset\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/twitter-sentiment/Sentiment Analysis Dataset 2.csv', on_bad_lines='skip')\n\n# Check the structure of the dataframe\nprint(df.head())\n\n# Clean text function\ndef clean_text(text):\n    # Remove URLs, mentions, hashtags, and special characters\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    return text\n\n# Apply cleaning to the 'SentimentText' column\ndf['cleaned_text'] = df['SentimentText'].apply(clean_text)\n\n# Convert sentiment labels to numerical format\ndf['Sentiment'] = df['Sentiment'].replace({0: 0, 2: 1, 4: 2})  # Adjust based on your labeling scheme\n\n# Step 2: Tokenize text and pad sequences\nmax_length = 100  # Maximum length of sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(df['cleaned_text'])\nsequences = tokenizer.texts_to_sequences(df['cleaned_text'])\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Step 3: Split data into training, validation, and testing sets\nX = padded_sequences\ny = df['Sentiment'].values\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split the temp set into validation and test sets\n\n# Step 4: Load pre-trained GloVe embeddings\nembeddings_index = {}\nglove_file = '/kaggle/input/glove-text-file/glove.6B.100d.txt'  # Adjust the path to your GloVe file\nwith open(glove_file, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nembedding_dim = 100\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Step 5: Build LSTM model with GloVe embeddings\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n                    output_dim=embedding_dim, \n                    weights=[embedding_matrix], \n                    input_length=max_length, \n                    trainable=False))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))  # 3 classes (0, 1, 2)\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Step 6: Train the model with validation data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3)\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=64, \n                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n\n# Step 7: Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.2f}')\n\n# Sample Input (Tweet Text)\nsample_tweet = \"I really enjoyed the movie, it was fantastic!\"\nsample_cleaned = clean_text(sample_tweet)\nsample_sequence = tokenizer.texts_to_sequences([sample_cleaned])\nsample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post')\n\n# Predict sentiment for the sample input\nsample_prediction = model.predict(sample_padded)\nprint(f'Sample Tweet Sentiment: {np.argmax(sample_prediction)}')  # Output the sentiment class\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T12:09:30.761215Z","iopub.execute_input":"2024-10-18T12:09:30.761809Z","iopub.status.idle":"2024-10-18T16:07:52.506682Z","shell.execute_reply.started":"2024-10-18T12:09:30.761765Z","shell.execute_reply":"2024-10-18T16:07:52.505228Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"   ItemID  Sentiment SentimentSource  \\\n0       1          0    Sentiment140   \n1       2          0    Sentiment140   \n2       3          1    Sentiment140   \n3       4          0    Sentiment140   \n4       5          0    Sentiment140   \n\n                                       SentimentText  \n0                       is so sad for my APL frie...  \n1                     I missed the New Moon trail...  \n2                            omg its already 7:30 :O  \n3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n4           i think mi bf is cheating on me!!!   ...  \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n\u001b[1m19733/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4646s\u001b[0m 235ms/step - accuracy: 0.5000 - loss: 0.6968 - val_accuracy: 0.5002 - val_loss: 0.6932\nEpoch 2/3\n\u001b[1m19733/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4696s\u001b[0m 238ms/step - accuracy: 0.5008 - loss: 0.6932 - val_accuracy: 0.5002 - val_loss: 0.6931\nEpoch 3/3\n\u001b[1m19733/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4568s\u001b[0m 231ms/step - accuracy: 0.4999 - loss: 0.6932 - val_accuracy: 0.4998 - val_loss: 0.6932\n\u001b[1m4934/4934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 53ms/step - accuracy: 0.5025 - loss: 0.6932\nTest Accuracy: 0.50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step\nSample Tweet Sentiment: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"C. Movie Reviews Sentiment Classification with BERT ","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/input/'))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T16:08:13.320434Z","iopub.execute_input":"2024-10-18T16:08:13.320909Z","iopub.status.idle":"2024-10-18T16:08:13.327579Z","shell.execute_reply.started":"2024-10-18T16:08:13.320867Z","shell.execute_reply":"2024-10-18T16:08:13.326427Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['using-word-embeddings-for-sentiment-analysis', 'twitter-entity-sentiment-analysis', 'twitter-sentiment-analysis-using-tensorflow', 'imdb-review', 'twitter-sentiment', 'movie-review', 'glove-text-file']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Step 1: Load and preprocess the dataset\n# Load the dataset \ndf = pd.read_csv('/kaggle/input/imdb-review/imdb_reviews.csv')  # Adjust to your actual dataset path\n\n# Display the first few rows of the dataset\nprint(df.head())\n\n# Convert sentiment labels to numerical format (0 for negative, 1 for positive)\ndf['sentiment'] = df['sentiment'].map({'neg': 0, 'pos': 1})\n\n# Clean text function (optional, based on dataset specifics)\ndef clean_text(text):\n    # Here, you can add any cleaning steps if necessary (e.g., removing special characters)\n    return text\n\n# Apply text cleaning\ndf['text'] = df['text'].apply(clean_text)\n\n# Step 2: Tokenize and encode reviews using BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass IMDBDataset(Dataset):\n    def __init__(self, reviews, labels):\n        self.reviews = reviews\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = self.reviews[idx]\n        label = self.labels[idx]\n\n        # Tokenize and encode the review\n        encoding = tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=256,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Step 3: Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = IMDBDataset(X_train.to_numpy(), y_train.to_numpy())\ntest_dataset = IMDBDataset(X_test.to_numpy(), y_test.to_numpy())\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Step 4: Load pre-trained BERT model and fine-tune for sentiment classification\n# Load tokenizer from a local directory where BERT files are stored\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', local_files_only=True)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Train the model\nmodel.train()\nfor epoch in range(3):  # Adjust number of epochs as necessary\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch + 1}/{3}, Loss: {total_loss / len(train_loader)}')\n\n# Step 5: Evaluate the model\nmodel.eval()\npredictions, true_labels = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n        predictions.extend(preds)\n        true_labels.extend(batch['labels'].cpu().numpy())\n\n# Calculate accuracy and classification report\naccuracy = accuracy_score(true_labels, predictions)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n\n# Sample Input\nsample_review = \"The movie was boring and uninteresting.\"\nencoded_sample = tokenizer.encode_plus(\n    sample_review,\n    add_special_tokens=True,\n    max_length=256,\n    return_token_type_ids=False,\n    padding='max_length',\n    truncation=True,\n    return_attention_mask=True,\n    return_tensors='pt',\n)\n\n# Make prediction on the sample input\nmodel.eval()\nwith torch.no_grad():\n    input_ids = encoded_sample['input_ids'].to(device)\n    attention_mask = encoded_sample['attention_mask'].to(device)\n    output = model(input_ids, attention_mask=attention_mask)\n    prediction = torch.argmax(output.logits, dim=1).cpu().numpy()\n\nprint(f'Sample Input: \"{sample_review}\"')\nprint(f'Expected Output: {\"Positive\" if prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T16:18:56.168880Z","iopub.execute_input":"2024-10-18T16:18:56.169368Z","iopub.status.idle":"2024-10-18T16:20:57.262763Z","shell.execute_reply.started":"2024-10-18T16:18:56.169325Z","shell.execute_reply":"2024-10-18T16:20:57.261050Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   id                                               text sentiment\n0   1  Once again Mr. Costner has dragged out a movie...       neg\n1   2  This is an example of why the majority of acti...       neg\n2   3  First of all I hate those moronic rappers, who...       neg\n3   4  Not even the Beatles could write songs everyon...       neg\n4   5  Brass pictures movies is not a fitting word fo...       neg\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Step 2: Tokenize and encode reviews using BERT tokenizer\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIMDBDataset\u001b[39;00m(Dataset):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, reviews, labels):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2200\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2205\u001b[0m     )\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."],"ename":"OSError","evalue":"Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}